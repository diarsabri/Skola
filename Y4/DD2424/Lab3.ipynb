{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# DD2424 Deep Learning in Data Science - Lab 3\r\n",
    "\r\n",
    "## Diar Sabri - July 2021"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# imports\r\n",
    "import numpy as np\r\n",
    "import pickle\r\n",
    "import decimal\r\n",
    "from google.colab import drive\r\n",
    "drive.mount('/content/drive')\r\n",
    "import copy\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import math\r\n",
    "from scipy.linalg import fractional_matrix_power"
   ],
   "outputs": [],
   "metadata": {
    "id": "Ij0hMR9twswD",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ef4ffd8e-c8f4-47c5-ca6e-73ed2d35bf21"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def LoadBatch(filename):\r\n",
    "  \"\"\" Copied from the dataset website \"\"\"\r\n",
    "  with open('/content/drive/MyDrive/Colab Notebooks/cifar-10-batches-py/'+filename, 'rb') as fo:\r\n",
    "    dict = pickle.load(fo, encoding='bytes')\r\n",
    "\r\n",
    "  X = (dict[b'data'] / 255).T\r\n",
    "  y = dict[b'labels']\r\n",
    "  Y = (np.eye(10)[y]).T\r\n",
    "\r\n",
    "  X = PreProcessData(X)\r\n",
    "\r\n",
    "  return X,Y,y"
   ],
   "outputs": [],
   "metadata": {
    "id": "DrOJpOJbw_mr"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def LoadAllData():\r\n",
    "  X = []\r\n",
    "  Y = []\r\n",
    "  y = []\r\n",
    "\r\n",
    "  for i in range(1, 6):\r\n",
    "    filename = 'data_batch_' + str(i)\r\n",
    "    X_bat, Y_bat, y_bat = LoadBatch(filename)\r\n",
    "    X.append(X_bat)\r\n",
    "    Y.append(Y_bat)\r\n",
    "    y.append(y_bat)\r\n",
    "  \r\n",
    "  return np.concatenate(X, axis=1), np.concatenate(Y, axis=1), np.concatenate(y, axis=0)"
   ],
   "outputs": [],
   "metadata": {
    "id": "uqwYYBH1TodF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def PreProcessData(X):\r\n",
    "  mean_X = np.mean(X, axis=1).reshape(-1, 1)\r\n",
    "  std_X = np.std(X, axis=1).reshape(-1, 1)\r\n",
    "\r\n",
    "  X = (X - mean_X)/std_X\r\n",
    "\r\n",
    "  return X"
   ],
   "outputs": [],
   "metadata": {
    "id": "EuTjN4I3xBsr"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def InitalizeLayers(hidden, d, K, alpha, vrs = []):\r\n",
    "  layers = [item for sublist in [[d], hidden, [K]] for item in sublist]\r\n",
    "  k = len(layers)-1\r\n",
    "  NetParams = {}\r\n",
    "  NetParams['alpha'] = alpha\r\n",
    "  W = np.zeros(shape=(k,1)).tolist()\r\n",
    "  b = np.zeros(shape=(k,1)).tolist()\r\n",
    "\r\n",
    "\r\n",
    "  for i in range(0,k):\r\n",
    "    std = np.sqrt(2 / layers[i])\r\n",
    "\r\n",
    "    W[i] = np.random.normal(0, std, (layers[i+1],layers[i]))\r\n",
    "    b[i] = np.zeros(shape=(layers[i+1],1))\r\n",
    "\r\n",
    "  NetParams['gamma'] = np.zeros(shape=(len(hidden),1)).tolist()\r\n",
    "  NetParams['beta'] = np.zeros(shape=(len(hidden),1)).tolist()\r\n",
    "\r\n",
    "  for i in range(0,len(hidden)):\r\n",
    "    NetParams['gamma'][i] = np.ones(shape=(hidden[i],1))#np.ones(hidden[i])\r\n",
    "    NetParams['beta'][i] = np.zeros(shape=(hidden[i],1))#np.zeros(hidden[i])\r\n",
    "\r\n",
    "  NetParams['W'] = W\r\n",
    "  NetParams['b'] = b\r\n",
    "\r\n",
    "  return NetParams"
   ],
   "outputs": [],
   "metadata": {
    "id": "kNWyMJPOxBoD"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def MiniBatchGD(tra_X, tra_Y, GDP, NetParams, lamb):\r\n",
    "\r\n",
    "  training_loss, validation_loss = [], []\r\n",
    "  \r\n",
    "  D_size = len(tra_X[1])\r\n",
    "  etas = CyclicalLRList(GDP['eta_min'], GDP['eta_max'], GDP['n_s'])\r\n",
    "\r\n",
    "  for i in range(GDP['n_cycles']):\r\n",
    "    GDP['c'] = GDP['c'] + 1\r\n",
    "    for j in range(2*GDP['n_s']):\r\n",
    "      j_start = (j * GDP['n_batch']) % D_size\r\n",
    "      j_end = ((j + 1)* GDP['n_batch'] - 1) % D_size\r\n",
    "\r\n",
    "      X_batch = tra_X[:, j_start:j_end+1]\r\n",
    "      Y_batch = tra_Y[:, j_start:j_end+1]\r\n",
    "      GDP['t'] = GDP['t'] + 1\r\n",
    "\r\n",
    "      BNParams = ForwardPass(X_batch, NetParams)\r\n",
    "      grad_W, grad_b = BackwardPass(X_batch, Y_batch, BNParams, NetParams, lamb)\r\n",
    "\r\n",
    "      # Record the loss at the breaks between different epochs\r\n",
    "      if j_start == 0 or j == 2*GDP['n_s'] - 1:\r\n",
    "        global_step = i * 2 * GDP['n_s'] + j\r\n",
    "        tra_loss = (global_step, ComputeCostVanilla(tra_X, tra_Y, NetParams, lamb))\r\n",
    "        val_loss = (global_step, ComputeCostVanilla(GDP['val_X'], GDP['val_Y'], NetParams, lamb))\r\n",
    "\r\n",
    "        training_loss.append(tra_loss)\r\n",
    "        validation_loss.append(val_loss)\r\n",
    "\r\n",
    "      # Update gradients\r\n",
    "      cyclic_eta = etas[j]\r\n",
    "      for q in range(len(NetParams['W'])):\r\n",
    "        NetParams['W'][q] = NetParams['W'][q] - cyclic_eta * grad_W[q]\r\n",
    "        NetParams['b'][q] = NetParams['b'][q] - cyclic_eta * grad_b[q]\r\n",
    "\r\n",
    "  return NetParams, training_loss, validation_loss"
   ],
   "outputs": [],
   "metadata": {
    "id": "e0emvNxCjLYm"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def ComputeCostVanilla(X, Y, NetParams, lamb):\r\n",
    "  eval = ForwardPass(X, NetParams)\r\n",
    "  tot_sum = 0\r\n",
    "  for weight_matrix in NetParams['W']:\r\n",
    "      tot_sum += np.sum(np.square(weight_matrix))\r\n",
    "\r\n",
    "  loss = (1/X.shape[1]) * -np.sum(Y*np.log(eval['P']))\r\n",
    "\r\n",
    "  j = loss + lamb * tot_sum\r\n",
    "\r\n",
    "  return loss, j"
   ],
   "outputs": [],
   "metadata": {
    "id": "j-hBDKP-m08A"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def ForwardPass(X, NetParams):\r\n",
    "  W = NetParams['W']\r\n",
    "  b = NetParams['b']\r\n",
    "\r\n",
    "  k = len(W) - 1\r\n",
    "  x = X\r\n",
    "  hidden = []\r\n",
    "\r\n",
    "  for i in range(k):\r\n",
    "    s = W[i] @ x + b[i]\r\n",
    "    s[s < 0] = 0\r\n",
    "    x = copy.deepcopy(s)\r\n",
    "    hidden.append(copy.deepcopy(s))\r\n",
    "\r\n",
    "  x = W[-1] @ x + b[-1]\r\n",
    "  P = np.exp(x) / np.sum(np.exp(x), axis=0)\r\n",
    "\r\n",
    "  BNParams = {}\r\n",
    "  BNParams['P'] = P\r\n",
    "  BNParams['hidden'] = hidden\r\n",
    "\r\n",
    "  return BNParams"
   ],
   "outputs": [],
   "metadata": {
    "id": "8IqN5UMsgSGh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def BackwardPass(X, Y, BNParams, NetParams ,lam):\r\n",
    "  W = NetParams['W']\r\n",
    "  b = NetParams['b']\r\n",
    "  P = BNParams['P']\r\n",
    "  hidden = BNParams['hidden']\r\n",
    "\r\n",
    "  G = -(Y - P)\r\n",
    "\r\n",
    "  k = len(W) - 1\r\n",
    "  grad_W = []\r\n",
    "  grad_b = []\r\n",
    "\r\n",
    "  for l in range(k, 0, -1):\r\n",
    "    grad_W.append(G @ hidden[l-1].T / X.shape[1] + 2 * lam * W[l])\r\n",
    "    grad_b.append(np.mean(G, axis=-1, keepdims=True))\r\n",
    "\r\n",
    "    G = W[l].T @ G\r\n",
    "    G = np.multiply(G, hidden[l-1] > 0)\r\n",
    "\r\n",
    "  w = G @ X.T / X.shape[1] + 2 * lam * W[0]\r\n",
    "  b = np.mean(G, axis=-1, keepdims=True)\r\n",
    "\r\n",
    "  grad_W.append(w)\r\n",
    "  grad_b.append(b)\r\n",
    "\r\n",
    "  grad_W.reverse()\r\n",
    "  grad_b.reverse()\r\n",
    "\r\n",
    "  return grad_W, grad_b"
   ],
   "outputs": [],
   "metadata": {
    "id": "lXSUi9SNhFiD"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def MiniBatchGDBN(tra_X, tra_Y, tra_y, GDP, NetParams, lamb):\r\n",
    "\r\n",
    "  # Init the means and variances before the main loop\r\n",
    "  BNParams = ForwardPassBN(tra_X[:, :100], NetParams)\r\n",
    "  my_avg = BNParams['my']\r\n",
    "  v_avg = BNParams['v']\r\n",
    "\r\n",
    "  training_loss, validation_loss = [], []\r\n",
    "  \r\n",
    "  D_size = len(tra_X[1])\r\n",
    "  etas = CyclicalLRList(GDP['eta_min'], GDP['eta_max'], GDP['n_s'])\r\n",
    "\r\n",
    "  for i in range(GDP['n_cycles']):\r\n",
    "    GDP['c'] = GDP['c'] + 1\r\n",
    "    for j in range(2*GDP['n_s']):\r\n",
    "      j_start = (j * GDP['n_batch']) % D_size\r\n",
    "      j_end = ((j + 1)* GDP['n_batch'] - 1) % D_size\r\n",
    "\r\n",
    "      X_batch = tra_X[:, j_start:j_end+1]\r\n",
    "      Y_batch = tra_Y[:, j_start:j_end+1]\r\n",
    "      GDP['t'] = GDP['t'] + 1\r\n",
    "\r\n",
    "      BNParams = ForwardPassBN(X_batch, NetParams)\r\n",
    "      grad_W, grad_b, grad_beta, grad_gamma = BackwardPassBN(X_batch, Y_batch, BNParams, NetParams, lamb)\r\n",
    "\r\n",
    "      # Record the loss at the breaks between different epochs\r\n",
    "      if j_start == 0 or j == 2*GDP['n_s'] - 1:\r\n",
    "        global_step = i * 2 * GDP['n_s'] + j\r\n",
    "\r\n",
    "        tra_loss = ComputeCostWAcc(tra_X, tra_Y, tra_y, NetParams, my_avg, v_avg, lamb)\r\n",
    "        val_loss = ComputeCostWAcc(GDP['val_X'], GDP['val_Y'], GDP['val_y'], NetParams, my_avg, v_avg, lamb)\r\n",
    "        \r\n",
    "        tra_loss = (global_step, (tra_loss[0], tra_loss[1]))\r\n",
    "        val_loss = (global_step, (val_loss[0], val_loss[1]))\r\n",
    "\r\n",
    "        training_loss.append(tra_loss)\r\n",
    "        validation_loss.append(val_loss)\r\n",
    "\r\n",
    "      # Update avg means and variances\r\n",
    "      for k in range(len(my_avg)):\r\n",
    "        my_avg[k] = NetParams['alpha'] * my_avg[k] + (1 - NetParams['alpha']) * BNParams['my'][k]\r\n",
    "        v_avg[k] = NetParams['alpha'] * v_avg[k] + (1 - NetParams['alpha']) * BNParams['v'][k]\r\n",
    "\r\n",
    "      # Update weight matrixes\r\n",
    "      cyclic_eta = etas[j]\r\n",
    "      for q in range(len(NetParams['W'])):\r\n",
    "        NetParams['W'][q] = NetParams['W'][q] - cyclic_eta * grad_W[q]\r\n",
    "        NetParams['b'][q] = NetParams['b'][q] - cyclic_eta * grad_b[q]\r\n",
    "        if q < len(grad_W) - 1:\r\n",
    "          NetParams['beta'][q] = NetParams['beta'][q] - cyclic_eta * grad_beta[q]\r\n",
    "          NetParams['gamma'][q] = NetParams['gamma'][q] - cyclic_eta * grad_gamma[q]\r\n",
    "          \r\n",
    "  return NetParams, training_loss, validation_loss"
   ],
   "outputs": [],
   "metadata": {
    "id": "MA_hCDNOxQMb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def CyclicalLRList(eta_min, eta_max, n_s):\r\n",
    "  diff = eta_max - eta_min\r\n",
    "  etas = []\r\n",
    "  for t in range(2 * n_s):\r\n",
    "    val = eta_min + t/n_s * diff if t <= n_s else eta_max - (t - n_s)/n_s * diff\r\n",
    "    etas.append(val)\r\n",
    "  \r\n",
    "  return etas"
   ],
   "outputs": [],
   "metadata": {
    "id": "Tp3PkbbYyE0H"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def ForwardPassBN(X, NetParams, MU=None, V=None):\r\n",
    "  if MU is None:\r\n",
    "    MU = []\r\n",
    "  if V is None:\r\n",
    "    V = []\r\n",
    "  \r\n",
    "  W = NetParams['W']\r\n",
    "  b = NetParams['b']\r\n",
    "  beta = NetParams['beta']\r\n",
    "  gamma = NetParams['gamma']\r\n",
    "  \r\n",
    "  test_time = len(MU) > 0 and len(V) > 0\r\n",
    "  k = len(W) - 1\r\n",
    "\r\n",
    "  S = []\r\n",
    "  S_hat = []\r\n",
    "  x = [X]\r\n",
    "  for l in range(k):\r\n",
    "    s = W[l] @ x[l] + b[l]\r\n",
    "    S.append(s)\r\n",
    "\r\n",
    "    if test_time:\r\n",
    "      mu = MU[l]\r\n",
    "      v = V[l]\r\n",
    "    else:\r\n",
    "      mu = np.mean(s, axis=1, keepdims=True)\r\n",
    "      MU.append(mu)\r\n",
    "      v = np.var(s, axis=1, keepdims=True)\r\n",
    "      V.append(v)\r\n",
    "\r\n",
    "    s_hat = BatchNormalize(s, mu, v)\r\n",
    "    S_hat.append(s_hat)\r\n",
    "\r\n",
    "    s_scaled = np.multiply(gamma[l], s_hat) + beta[l]\r\n",
    "    \r\n",
    "    s_scaled[s_scaled < 0] = 0\r\n",
    "    x.append(s_scaled)\r\n",
    "\r\n",
    "  s = W[-1] @ x[-1] + b[-1]\r\n",
    "  P = np.exp(x) / np.sum(np.exp(x), axis=0)\r\n",
    "\r\n",
    "  BNParams = {}\r\n",
    "  BNParams['s'] = S\r\n",
    "  BNParams['s_h'] = S_hat\r\n",
    "  BNParams['x'] = x\r\n",
    "  BNParams['my'] = MU\r\n",
    "  BNParams['v'] = V\r\n",
    "  BNParams['P'] = P\r\n",
    "\r\n",
    "  return BNParams"
   ],
   "outputs": [],
   "metadata": {
    "id": "F9G92XnByLJd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def BatchNormalize(s, mu, v):\r\n",
    "  v_flat_eps = v.flatten() + np.finfo(float).eps\r\n",
    "  return fractional_matrix_power(np.diag(v_flat_eps), -0.5) @ (s - mu)"
   ],
   "outputs": [],
   "metadata": {
    "id": "gEJjOw75yOLF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def BackwardPassBN(X, Y, BNParams, NetParams ,lam):\r\n",
    "  W = NetParams['W']\r\n",
    "  b = NetParams['b']\r\n",
    "  gamma = NetParams['gamma']\r\n",
    "  beta = NetParams['beta']\r\n",
    "\r\n",
    "  S = BNParams['s']\r\n",
    "  S_hat = BNParams['s_h']\r\n",
    "  X_batch = BNParams['x']\r\n",
    "  MU = BNParams['my']\r\n",
    "  V = BNParams['v']\r\n",
    "  P = BNParams['P']\r\n",
    "\r\n",
    "  n = X.shape[1]\r\n",
    "  ones = np.ones(n).reshape(-1, 1)\r\n",
    "  \r\n",
    "  G = -(Y - P)\r\n",
    "  w_k_grad = 1 / n * G @ X_batch[-1].T + 2 * lam * W[-1]\r\n",
    "  b_k_grad = 1 / n * G @ ones\r\n",
    "\r\n",
    "  G = W[-1].T @ G\r\n",
    "  G = np.multiply(G, X_batch[-1] > 0)\r\n",
    "\r\n",
    "  k = len(W) - 1\r\n",
    "  grad_W = [w_k_grad]\r\n",
    "  grad_b = [b_k_grad]\r\n",
    "  grad_beta = []\r\n",
    "  grad_gamma = []\r\n",
    "  for l in range(k-1, -1, -1):\r\n",
    "    grad_beta.append(1 / n * G @ ones)\r\n",
    "    grad_gamma.append(1 / n * np.multiply(G, S_hat[l]) @ ones)\r\n",
    "\r\n",
    "    G = np.multiply(G, np.reshape(gamma[l],(-1,1)) @ ones.T)\r\n",
    "    G = BNBackPass(G, S[l], MU[l], V[l])\r\n",
    "\r\n",
    "    grad_W.append(1 / n * G @ X_batch[l].T + 2 * lam * W[l])\r\n",
    "    grad_b.append(1 / n * G @ ones)\r\n",
    "\r\n",
    "    if l > 0:\r\n",
    "      G = W[l].T @ G\r\n",
    "      G = np.multiply(G, X_batch[l] > 0)\r\n",
    "\r\n",
    "  grad_W.reverse()\r\n",
    "  grad_b.reverse()\r\n",
    "  grad_beta.reverse()\r\n",
    "  grad_gamma.reverse()\r\n",
    "\r\n",
    "  return grad_W, grad_b, grad_beta, grad_gamma"
   ],
   "outputs": [],
   "metadata": {
    "id": "O192HusdyWnN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def BNBackPass(G, S, mu, v):\r\n",
    "  n = S.shape[1]\r\n",
    "  eps = np.finfo(float).eps\r\n",
    "  ones = np.ones(n).reshape(-1, 1)\r\n",
    "  sigma_one = np.array([pow((v_i + eps), -0.5) for v_i in v])\r\n",
    "  sigma_two = np.array([pow((v_i + eps), -1.5) for v_i in v])\r\n",
    "  \r\n",
    "  G1 = np.multiply(G, sigma_one @ ones.T)\r\n",
    "  G2 = np.multiply(G, sigma_two @ ones.T)\r\n",
    "  D = S - mu @ ones.T\r\n",
    "  c = np.multiply(G2, D) @ ones\r\n",
    "\r\n",
    "  return G1 - 1 / n * (G1 @ ones) @ ones.T - 1/n * np.multiply(D, c @ ones.T)  "
   ],
   "outputs": [],
   "metadata": {
    "id": "Dgokmgdgyai2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def ComputeCostWAcc(X, Y, y, NetParams, my_avg, v_avg, lamb):\r\n",
    "  eval = ForwardPassBN(X, NetParams, [my_avg, v_avg])\r\n",
    "  tot_sum = 0\r\n",
    "  for weight_matrix in NetParams['W']:\r\n",
    "      tot_sum += np.sum(np.square(weight_matrix))\r\n",
    "\r\n",
    "  loss = (1/X.shape[1]) * -np.sum(Y*np.log(eval['P']))\r\n",
    "  j = loss + lamb * tot_sum\r\n",
    "\r\n",
    "  acc = ComputeAcc(y, eval['P'])\r\n",
    "  return loss, j, acc"
   ],
   "outputs": [],
   "metadata": {
    "id": "jpNLeTmSyhR2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def ComputeCost(X, Y, NetParams, lamb):\r\n",
    "  eval = ForwardPassBN(X, NetParams)\r\n",
    "  tot_sum = 0\r\n",
    "  for weight_matrix in NetParams['W']:\r\n",
    "      tot_sum += np.sum(np.square(weight_matrix))\r\n",
    "\r\n",
    "  loss = (1/X.shape[1]) * -np.sum(Y*np.log(eval['P']))\r\n",
    "  j = loss + lamb * tot_sum\r\n",
    "\r\n",
    "  return loss, j"
   ],
   "outputs": [],
   "metadata": {
    "id": "yoCEBFs4kF7F"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def ComputeAcc(y, P):\r\n",
    "  preds = [np.argmax(pred) for pred in P.T]\r\n",
    "  corr = sum([1 for i in range(len(y)) if preds[i] == y[i]])\r\n",
    "  acc = corr/len(y) \r\n",
    "\r\n",
    "  return acc"
   ],
   "outputs": [],
   "metadata": {
    "id": "4UFedn3Uyq7W"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def PlotLoss(training_loss, validation_loss):\r\n",
    "\r\n",
    "  tra_X = [x for x, (_, y) in training_loss]\r\n",
    "  tra_Y = [y for x, (_, y) in training_loss]\r\n",
    "\r\n",
    "  val_X = [x for x, (_, y) in validation_loss]\r\n",
    "  val_Y = [y for x, (_, y) in validation_loss]\r\n",
    "\r\n",
    "  plt.plot(tra_X, tra_Y, label='Traning')\r\n",
    "  plt.plot(val_X, val_Y, label='Validation')\r\n",
    "  plt.legend()\r\n",
    "\r\n",
    "  plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "id": "uSqJvQzGad5A"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gradients"
   ],
   "metadata": {
    "id": "ieUPKDLnPEZQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def ComputeGradsNumSlowBN(X, Y, BNParams, NetParams, lam, h):\r\n",
    "  grad_W = []\r\n",
    "  grad_b = []\r\n",
    "  grad_beta = []\r\n",
    "  grad_gamma = []\r\n",
    "\r\n",
    "  for l in range(len(NetParams['W'])):\r\n",
    "    grad_W.append(np.zeros(NetParams['W'][l].shape))\r\n",
    "    grad_b.append(np.zeros(NetParams['b'][l].shape))\r\n",
    "\r\n",
    "    for i in range(len(NetParams['b'][l])):\r\n",
    "      NetParams_try = copy.deepcopy(NetParams)\r\n",
    "      NetParams_try['b'][l][i] -= h\r\n",
    "      _, c1 = ComputeCost(X, Y, NetParams_try, lam)\r\n",
    "\r\n",
    "      NetParams_try = copy.deepcopy(NetParams)\r\n",
    "      NetParams_try['b'][l][i] += h\r\n",
    "      _, c2 = ComputeCost(X, Y, NetParams_try, lam)\r\n",
    "\r\n",
    "      grad_b[l][i] = (c2 - c1) / (2 * h)\r\n",
    "\r\n",
    "    for i in range(NetParams['W'][l].shape[0]):\r\n",
    "      for j in range(NetParams['W'][l].shape[1]):\r\n",
    "        NetParams_try = copy.deepcopy(NetParams)\r\n",
    "        NetParams_try['W'][l][i, j] -= h\r\n",
    "        _, c1 = ComputeCost(X, Y, NetParams_try, lam)\r\n",
    "\r\n",
    "        NetParams_try = copy.deepcopy(NetParams)\r\n",
    "        NetParams_try['W'][l][i, j] += h\r\n",
    "        _, c2 = ComputeCost(X, Y, NetParams_try, lam)\r\n",
    "        \r\n",
    "        grad_W[l][i,j] = (c2 - c1) / (2 * h)\r\n",
    "    \r\n",
    "    if l < len(NetParams['W']) - 1:\r\n",
    "      grad_beta.append(np.zeros((len(NetParams['beta'][l]), 1)))\r\n",
    "      grad_gamma.append(np.zeros((len(NetParams['gamma'][l]), 1)))\r\n",
    "\r\n",
    "      for i in range(len(NetParams['beta'][l])):\r\n",
    "        NetParams_try = copy.deepcopy(NetParams)\r\n",
    "        NetParams_try['beta'][l][i] -= h\r\n",
    "        _, c1 = ComputeCost(X, Y, NetParams_try, lam)\r\n",
    "\r\n",
    "        NetParams_try = copy.deepcopy(NetParams)\r\n",
    "        NetParams_try['beta'][l][i] += h\r\n",
    "        _, c2 = ComputeCost(X, Y, NetParams_try, lam)\r\n",
    "\r\n",
    "        grad_beta[l][i] = (c2 - c1) / (2 * h)\r\n",
    "      \r\n",
    "      for i in range(len(NetParams['gamma'][l])):\r\n",
    "        NetParams_try = copy.deepcopy(NetParams)\r\n",
    "        NetParams_try['gamma'][l][i] -= h\r\n",
    "        _, c1 = ComputeCost(X, Y, NetParams_try, lam)\r\n",
    "\r\n",
    "        NetParams_try = copy.deepcopy(NetParams)\r\n",
    "        NetParams_try['gamma'][l][i] += h\r\n",
    "        _, c2 = ComputeCost(X, Y, NetParams_try, lam)\r\n",
    "\r\n",
    "        grad_gamma[l][i] = (c2 - c1) / (2 * h)\r\n",
    "    \r\n",
    "  return grad_W, grad_b, grad_beta, grad_gamma"
   ],
   "outputs": [],
   "metadata": {
    "id": "Mopy729GXKUi"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def AbsDiff(analytical, numerical, tolerance=1e-05):\r\n",
    "  assert analytical.shape == numerical.shape\r\n",
    "  analytical = analytical.flatten()\r\n",
    "  numerical = numerical.flatten()\r\n",
    "  equal = True\r\n",
    "  for i in range(len(analytical)):\r\n",
    "    diff = abs(analytical[i] - numerical[i])\r\n",
    "    if diff >= tolerance:\r\n",
    "      print(analytical[i], numerical[i], diff)\r\n",
    "      equal = False\r\n",
    "  return equal"
   ],
   "outputs": [],
   "metadata": {
    "id": "Xy5c_Bgfz2rL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def CompareGradientsBN():\r\n",
    "  samples = 10\r\n",
    "  hidden = [50, 50]\r\n",
    "  d = 10\r\n",
    "  K = 10\r\n",
    "  layers = len(hidden)\r\n",
    "  alpha = 0.9\r\n",
    "  lam = 0\r\n",
    "  h = 1e-5\r\n",
    "  tolerance = 1e-5\r\n",
    "\r\n",
    "  X, Y, _ = LoadBatch('data_batch_1')\r\n",
    "  X = X[:d, :samples]\r\n",
    "  Y = Y[:, :samples]\r\n",
    "  \r\n",
    "  NetParams = InitalizeLayers(hidden, d, K, alpha)\r\n",
    "  BNParams = ForwardPassBN(X, NetParams)\r\n",
    "\r\n",
    "  grad_W, grad_b, grad_beta, grad_gamma = BackwardPassBN(X, Y, BNParams, NetParams, lam)\r\n",
    "  grad_W_num, grad_b_num, grad_beta_num, grad_gamma_num = ComputeGradsNumSlowBN(X, Y, BNParams, NetParams, lam, h)\r\n",
    "\r\n",
    "\r\n",
    "  layer_equal = []\r\n",
    "  for l in range(len(grad_W)):\r\n",
    "    W_equal = AbsDiff(grad_W[l], grad_W_num[l], tolerance)\r\n",
    "    b_equal = AbsDiff(grad_b[l], grad_b_num[l], tolerance)\r\n",
    "    result = {\r\n",
    "      'W': W_equal,   \r\n",
    "      'b': b_equal,   \r\n",
    "    }\r\n",
    "\r\n",
    "    if l < len(grad_W) - 1:\r\n",
    "      beta_equal = AbsDiff(grad_beta[l], grad_beta_num[l], tolerance)\r\n",
    "      gamma_equal = AbsDiff(grad_gamma[l], grad_gamma_num[l], tolerance)\r\n",
    "      result['beta'] = beta_equal\r\n",
    "      result['gamma'] = gamma_equal\r\n",
    "\r\n",
    "    else:\r\n",
    "      result['beta'] = None\r\n",
    "      result['gamma'] = None\r\n",
    "    \r\n",
    "    layer_equal.append(result)\r\n",
    "    \r\n",
    "  grads = {}\r\n",
    "  grads['W'] = grad_W\r\n",
    "  grads['b'] = grad_b\r\n",
    "  grads['beta'] = grad_beta\r\n",
    "  grads['gamma'] = grad_gamma\r\n",
    "\r\n",
    "  grads_num = {}\r\n",
    "  grads_num['W'] = grad_W_num\r\n",
    "  grads_num['b'] = grad_b_num\r\n",
    "  grads_num['beta'] = grad_beta_num\r\n",
    "  grads_num['gamma'] = grad_gamma_num\r\n",
    "\r\n",
    "  return grads, grads_num, layer_equal"
   ],
   "outputs": [],
   "metadata": {
    "id": "1MeQIiWRXRaG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grads, grads_num, layers_compared = CompareGradientsBN()\r\n",
    "layers_compared"
   ],
   "outputs": [],
   "metadata": {
    "id": "CewhJhp5icaw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Batch-norm vs Normal"
   ],
   "metadata": {
    "id": "HqSmKXqUPH9l"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data"
   ],
   "metadata": {
    "id": "vfCloHcpVhUa"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_X, test_Y, test_y = LoadBatch('test_batch')\r\n",
    "all_X, all_Y, all_y = LoadAllData()"
   ],
   "outputs": [],
   "metadata": {
    "id": "TuGJZc7dTPvU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def PartData(validation_size):\r\n",
    "  val_X = all_X[:, :validation_size]\r\n",
    "  val_Y = all_Y[:, :validation_size]\r\n",
    "  val_y = all_y[:validation_size]\r\n",
    "\r\n",
    "  tra_X = all_X[:, validation_size:]\r\n",
    "  tra_Y = all_Y[:, validation_size:]\r\n",
    "  tra_y = all_y[validation_size:]\r\n",
    "\r\n",
    "  return val_X, val_Y, val_y, tra_X, tra_Y, tra_y"
   ],
   "outputs": [],
   "metadata": {
    "id": "y96-hvbcopH9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 Layers"
   ],
   "metadata": {
    "id": "N8p2XG8BPMtx"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#3 Layer experiment with no bn\r\n",
    "def Exp3LNoBN():\r\n",
    "  print('3 Layers - NO BATCH NORM')\r\n",
    "\r\n",
    "  validation_size = 5000\r\n",
    "  val_X, val_Y, val_y, tra_X, tra_Y, tra_y = PartData(validation_size)\r\n",
    "\r\n",
    "  d = len(tra_X)\r\n",
    "  K = len(tra_Y)\r\n",
    "\r\n",
    "  # The shape of the network \r\n",
    "  hidden = [50, 50]\r\n",
    "  alpha = 0.9\r\n",
    "  NetParams = InitalizeLayers(hidden, d, K, alpha)\r\n",
    "\r\n",
    "  # Hyper-paramters for the experiment\r\n",
    "  GDP = {}\r\n",
    "  GDP['n_cycles'] = 2\r\n",
    "  GDP['n_batch'] = 100\r\n",
    "  GDP['n_s'] = int(np.floor(5*45000 / GDP['n_batch']))\r\n",
    "  GDP['t'] = int(0)\r\n",
    "  GDP['c'] = int(0)\r\n",
    "  GDP['eta_min'] = 1e-5\r\n",
    "  GDP['eta_max'] = 1e-1\r\n",
    "  GDP['val_X'] = val_X\r\n",
    "  GDP['val_Y'] = val_Y\r\n",
    "  GDP['val_y'] = val_y\r\n",
    "\r\n",
    "  lamb = 0.005\r\n",
    "\r\n",
    "  NetParams, training_loss, validation_loss = MiniBatchGD(tra_X, tra_Y, GDP, NetParams, lamb)\r\n",
    "\r\n",
    "\r\n",
    "  P_test = ForwardPass(test_X, NetParams)['P']\r\n",
    "  acc_test = ComputeAcc(test_y, P_test)\r\n",
    "  print('Test accuracy: ' + str(acc_test))\r\n",
    "\r\n",
    "  PlotLoss(training_loss, validation_loss)\r\n",
    "\r\n",
    "  return NetParams, training_loss, validation_loss\r\n",
    "\r\n",
    "final_model, final_training_loss, final_validation_loss = Exp3LNoBN()"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "cellView": "code",
    "id": "WtGRmftCN1lm",
    "outputId": "d1ae97ad-4e83-4cfe-bcba-7050a5951504"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#3 Layer experiment with bn\r\n",
    "def Exp3LBN():\r\n",
    "  print('3 Layers - BATCH NORM')\r\n",
    "\r\n",
    "  validation_size = 5000\r\n",
    "  val_X, val_Y, val_y, tra_X, tra_Y, tra_y = PartData(validation_size)\r\n",
    "\r\n",
    "  d = len(tra_X)\r\n",
    "  K = len(tra_Y)\r\n",
    "\r\n",
    "  # The shape of the network \r\n",
    "  hidden = [50, 50]\r\n",
    "  alpha = 0.9\r\n",
    "  NetParams = InitalizeLayers(hidden, d, K, alpha)\r\n",
    "\r\n",
    "  # Hyper-paramters for the experiment\r\n",
    "  GDP = {}\r\n",
    "  GDP['n_cycles'] = 2\r\n",
    "  GDP['n_batch'] = 100\r\n",
    "  GDP['n_s'] = int(np.floor(5*45000 / GDP['n_batch']))\r\n",
    "  GDP['t'] = int(0)\r\n",
    "  GDP['c'] = int(0)\r\n",
    "  GDP['eta_min'] = 1e-5\r\n",
    "  GDP['eta_max'] = 1e-1\r\n",
    "  GDP['val_X'] = val_X\r\n",
    "  GDP['val_Y'] = val_Y\r\n",
    "  GDP['val_y'] = val_y\r\n",
    "\r\n",
    "  lamb = 0.005\r\n",
    "\r\n",
    "  NetParams, training_loss, validation_loss = MiniBatchGDBN(tra_X, tra_Y, tra_y, GDP, NetParams, lamb)\r\n",
    "\r\n",
    "  P_test = ForwardPassBN(test_X, NetParams)['P']\r\n",
    "  acc_test = ComputeAcc(test_y, P_test)\r\n",
    "  print('Test accuracy: ' + str(acc_test))\r\n",
    "\r\n",
    "  PlotLoss(training_loss, validation_loss)\r\n",
    "\r\n",
    "  return NetParams, training_loss, validation_loss\r\n",
    "\r\n",
    "final_model, final_training_loss, final_validation_loss = Exp3LBN()"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "cellView": "form",
    "id": "Z5nj-G6tdYwv",
    "outputId": "d7df5cbc-a97e-4f86-bb81-29c5839549a2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9 Layers"
   ],
   "metadata": {
    "id": "8oSH39p6bVBn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#9 Layer experiment with no bn\r\n",
    "def Exp9LNoBN():\r\n",
    "  print('9 Layers - NO BATCH NORM')\r\n",
    "\r\n",
    "  validation_size = 5000\r\n",
    "  val_X, val_Y, val_y, tra_X, tra_Y, tra_y = PartData(validation_size)\r\n",
    "\r\n",
    "  d = len(tra_X)\r\n",
    "  K = len(tra_Y)\r\n",
    "\r\n",
    "  # The shape of the network \r\n",
    "  hidden = [50, 30, 20, 20, 10, 10, 10, 10]\r\n",
    "  alpha = 0.9\r\n",
    "  NetParams = InitalizeLayers(hidden, d, K, alpha)\r\n",
    "\r\n",
    "  # Hyper-paramters for the experiment\r\n",
    "  GDP = {}\r\n",
    "  GDP['n_cycles'] = 2\r\n",
    "  GDP['n_batch'] = 100\r\n",
    "  GDP['n_s'] = int(np.floor(5*45000 / GDP['n_batch']))\r\n",
    "  GDP['t'] = int(0)\r\n",
    "  GDP['c'] = int(0)\r\n",
    "  GDP['eta_min'] = 1e-5\r\n",
    "  GDP['eta_max'] = 1e-1\r\n",
    "  GDP['val_X'] = val_X\r\n",
    "  GDP['val_Y'] = val_Y\r\n",
    "  GDP['val_y'] = val_y\r\n",
    "\r\n",
    "  lamb = 0.005\r\n",
    "\r\n",
    "  NetParams, training_loss, validation_loss = MiniBatchGD(tra_X, tra_Y, GDP, NetParams, lamb)\r\n",
    "\r\n",
    "  P_test = ForwardPass(test_X, NetParams)['P']\r\n",
    "  acc_test = ComputeAcc(test_y, P_test)\r\n",
    "  print('Test accuracy: ' + str(acc_test))\r\n",
    "\r\n",
    "  PlotLoss(training_loss, validation_loss)\r\n",
    "\r\n",
    "  return NetParams, training_loss, validation_loss\r\n",
    "\r\n",
    "final_model, final_training_loss, final_validation_loss = Exp9LNoBN()"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "cellView": "form",
    "id": "MWQoQtnUbYOM",
    "outputId": "5ceb1c67-2e79-4bbd-ddcb-853f375146c7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#9 Layer experiment with bn\r\n",
    "def Exp9LBN():\r\n",
    "  print('9 Layers - BATCH NORM')\r\n",
    "\r\n",
    "  validation_size = 5000\r\n",
    "  val_X, val_Y, val_y, tra_X, tra_Y, tra_y = PartData(validation_size)\r\n",
    "\r\n",
    "  d = len(tra_X)\r\n",
    "  K = len(tra_Y)\r\n",
    "\r\n",
    "  # The shape of the network \r\n",
    "  hidden = [50, 30, 20, 20, 10, 10, 10, 10]\r\n",
    "  alpha = 0.9\r\n",
    "  NetParams = InitalizeLayers(hidden, d, K, alpha)\r\n",
    "\r\n",
    "  # Hyper-paramters for the experiment\r\n",
    "  GDP = {}\r\n",
    "  GDP['n_cycles'] = 2\r\n",
    "  GDP['n_batch'] = 100\r\n",
    "  GDP['n_s'] = int(np.floor(5*45000 / GDP['n_batch']))\r\n",
    "  GDP['t'] = int(0)\r\n",
    "  GDP['c'] = int(0)\r\n",
    "  GDP['eta_min'] = 1e-5\r\n",
    "  GDP['eta_max'] = 1e-1\r\n",
    "  GDP['val_X'] = val_X\r\n",
    "  GDP['val_Y'] = val_Y\r\n",
    "  GDP['val_y'] = val_y\r\n",
    "\r\n",
    "  lamb = 0.005\r\n",
    "\r\n",
    "  NetParams, training_loss, validation_loss = MiniBatchGDBN(tra_X, tra_Y, tra_y, GDP, NetParams, lamb)\r\n",
    "\r\n",
    "  P_test = ForwardPassBN(test_X, NetParams)['P']\r\n",
    "  acc_test = ComputeAcc(test_y, P_test)\r\n",
    "  print('Test accuracy: ' + str(acc_test))\r\n",
    "\r\n",
    "  PlotLoss(training_loss, validation_loss)\r\n",
    "\r\n",
    "  return NetParams, training_loss, validation_loss\r\n",
    "\r\n",
    "final_model, final_training_loss, final_validation_loss = Exp9LBN()"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "cellView": "form",
    "id": "HxPNdrrc3Xfp",
    "outputId": "e6c0b79d-e7db-4425-b8d8-319b06960132"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lambda"
   ],
   "metadata": {
    "id": "FpeG3BwcQI4r"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Lambda Optimization\r\n",
    "def ExpLambdaOpt():\r\n",
    "  print('Lambda Optimization')\r\n",
    "\r\n",
    "  validation_size = 5000\r\n",
    "  val_X, val_Y, val_y, tra_X, tra_Y, tra_y = PartData(validation_size)\r\n",
    "\r\n",
    "  d = len(tra_X)\r\n",
    "  K = len(tra_Y)\r\n",
    "\r\n",
    "  # The shape of the network \r\n",
    "  hidden = [50, 50]\r\n",
    "  alpha = 0.9\r\n",
    "\r\n",
    "  # Hyper-paramters for the experiment\r\n",
    "  GDP = {}\r\n",
    "  GDP['n_cycles'] = 2\r\n",
    "  GDP['n_batch'] = 100\r\n",
    "  GDP['n_s'] = int(np.floor(5*45000 / GDP['n_batch']))\r\n",
    "  GDP['t'] = int(0)\r\n",
    "  GDP['c'] = int(0)\r\n",
    "  GDP['eta_min'] = 1e-5\r\n",
    "  GDP['eta_max'] = 1e-1\r\n",
    "  GDP['val_X'] = val_X\r\n",
    "  GDP['val_Y'] = val_Y\r\n",
    "  GDP['val_y'] = val_y\r\n",
    "\r\n",
    "  n_lambdas = 10\r\n",
    "  l_min = -5\r\n",
    "  l_max = -1\r\n",
    "  grid = np.linspace(0, 1, n_lambdas)\r\n",
    "  lambdas = [np.power(10, l_min + (l_max - l_min) * val) for val in grid]\r\n",
    "  \r\n",
    "  for lamb in lambdas:\r\n",
    "    NetParams = InitalizeLayers(hidden, d, K, alpha)\r\n",
    "    NetParams, _, _ = MiniBatchGDBN(tra_X, tra_Y, tra_y, GDP, NetParams, lamb)\r\n",
    "\r\n",
    "    P_test = ForwardPassBN(test_X, NetParams)['P']\r\n",
    "    acc_test = ComputeAcc(test_y, P_test)\r\n",
    "    print(str(lamb) + ' - Test accuracy: ' + str(acc_test))\r\n",
    "\r\n",
    "ExpLambdaOpt()"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "cellView": "form",
    "id": "hvKE92N-QK78",
    "outputId": "04249867-2023-43af-c37a-965a93ad1ec9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sensitivity to initialization"
   ],
   "metadata": {
    "id": "Jxj30AQLCIgC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def InitLFixedSigma(hidden, d, K, alpha, sigma, vrs = []):\r\n",
    "  layers = [item for sublist in [[d], hidden, [K]] for item in sublist]\r\n",
    "  k = len(layers)-1\r\n",
    "  NetParams = {}\r\n",
    "  NetParams['alpha'] = alpha\r\n",
    "  W = np.zeros(shape=(k,1)).tolist()\r\n",
    "  b = np.zeros(shape=(k,1)).tolist()\r\n",
    "\r\n",
    "\r\n",
    "  for i in range(0,k):\r\n",
    "    ##if not vrs:\r\n",
    "      ##std = np.sqrt(2/layers[i])\r\n",
    "    ##else:\r\n",
    "      ##std = vrs[0]\r\n",
    "\r\n",
    "    std = sigma\r\n",
    "\r\n",
    "    W[i] = np.random.normal(0, std, (layers[i+1],layers[i]))\r\n",
    "    b[i] = np.zeros(shape=(layers[i+1],1))\r\n",
    "\r\n",
    "  NetParams['gamma'] = np.zeros(shape=(len(hidden),1)).tolist()\r\n",
    "  NetParams['beta'] = np.zeros(shape=(len(hidden),1)).tolist()\r\n",
    "\r\n",
    "  for i in range(0,len(hidden)):\r\n",
    "    NetParams['gamma'][i] = np.ones(shape=(hidden[i],1))#np.ones(hidden[i])\r\n",
    "    NetParams['beta'][i] = np.zeros(shape=(hidden[i],1))#np.zeros(hidden[i])\r\n",
    "\r\n",
    "  NetParams['W'] = W\r\n",
    "  NetParams['b'] = b\r\n",
    "\r\n",
    "  return NetParams"
   ],
   "outputs": [],
   "metadata": {
    "id": "DO_ZeHbqCNap"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fixed sigma experiment with no bn\r\n",
    "def ExpFixedSigma():\r\n",
    "  print('Fixed Sigma - NO BN')\r\n",
    "\r\n",
    "  validation_size = 5000\r\n",
    "  val_X, val_Y, val_y, tra_X, tra_Y, tra_y = PartData(validation_size)\r\n",
    "\r\n",
    "  d = len(tra_X)\r\n",
    "  K = len(tra_Y)\r\n",
    "\r\n",
    "  # The shape of the network \r\n",
    "  hidden = [50, 50]\r\n",
    "  alpha = 0.9\r\n",
    "  lamb = 0.005\r\n",
    "\r\n",
    "  # Hyper-paramters for the experiment\r\n",
    "  GDP = {}\r\n",
    "  GDP['n_cycles'] = 2\r\n",
    "  GDP['n_batch'] = 100\r\n",
    "  GDP['n_s'] = int(np.floor(5*45000 / GDP['n_batch']))\r\n",
    "  GDP['t'] = int(0)\r\n",
    "  GDP['c'] = int(0)\r\n",
    "  GDP['eta_min'] = 1e-5\r\n",
    "  GDP['eta_max'] = 1e-1\r\n",
    "  GDP['val_X'] = val_X\r\n",
    "  GDP['val_Y'] = val_Y\r\n",
    "  GDP['val_y'] = val_y\r\n",
    "\r\n",
    "  sigmas = [1e-1, 1e-3, 1e-4]\r\n",
    "\r\n",
    "  for sigma in sigmas:\r\n",
    "    NetParams = InitLFixedSigma(hidden, d, K, alpha, sigma)\r\n",
    "    NetParams, training_loss, validation_loss = MiniBatchGD(tra_X, tra_Y, GDP, NetParams, lamb)\r\n",
    "\r\n",
    "    P_test = ForwardPassBN(test_X, NetParams)['P']\r\n",
    "    acc_test = ComputeAcc(test_y, P_test)\r\n",
    "    print(str(sigma) + ' - Test accuracy: ' + str(acc_test))\r\n",
    "\r\n",
    "    PlotLoss(training_loss, validation_loss)\r\n",
    "\r\n",
    "ExpFixedSigma()"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 846
    },
    "cellView": "form",
    "id": "1ounk5jMEMza",
    "outputId": "4220ce16-5dd1-48d5-cee7-b9b51a4c643d"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fixed sigma experiment with bn\r\n",
    "def ExpFixedSigmaBN():\r\n",
    "  print('Fixed Sigma - BN')\r\n",
    "\r\n",
    "  validation_size = 5000\r\n",
    "  val_X, val_Y, val_y, tra_X, tra_Y, tra_y = PartData(validation_size)\r\n",
    "\r\n",
    "  d = len(tra_X)\r\n",
    "  K = len(tra_Y)\r\n",
    "\r\n",
    "  # The shape of the network \r\n",
    "  hidden = [50, 50]\r\n",
    "  alpha = 0.9\r\n",
    "  lamb = 0.005\r\n",
    "\r\n",
    "  # Hyper-paramters for the experiment\r\n",
    "  GDP = {}\r\n",
    "  GDP['n_cycles'] = 2\r\n",
    "  GDP['n_batch'] = 100\r\n",
    "  GDP['n_s'] = int(np.floor(5*45000 / GDP['n_batch']))\r\n",
    "  GDP['t'] = int(0)\r\n",
    "  GDP['c'] = int(0)\r\n",
    "  GDP['eta_min'] = 1e-5\r\n",
    "  GDP['eta_max'] = 1e-1\r\n",
    "  GDP['val_X'] = val_X\r\n",
    "  GDP['val_Y'] = val_Y\r\n",
    "  GDP['val_y'] = val_y\r\n",
    "\r\n",
    "  sigmas = [1e-1, 1e-3, 1e-4]\r\n",
    "\r\n",
    "  for sigma in sigmas:\r\n",
    "    NetParams = InitLFixedSigma(hidden, d, K, alpha, sigma)\r\n",
    "    NetParams, training_loss, validation_loss = MiniBatchGDBN(tra_X, tra_Y, tra_y, GDP, NetParams, lamb)\r\n",
    "\r\n",
    "    P_test = ForwardPassBN(test_X, NetParams)['P']\r\n",
    "    acc_test = ComputeAcc(test_y, P_test)\r\n",
    "    print(str(sigma) + ' - Test accuracy: ' + str(acc_test))\r\n",
    "\r\n",
    "    PlotLoss(training_loss, validation_loss)\r\n",
    "\r\n",
    "ExpFixedSigmaBN()"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 834
    },
    "cellView": "form",
    "id": "NciZ2LNqCh3Y",
    "outputId": "bc8de874-f903-46c4-caea-7c09a19d7352"
   }
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}